{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d155d2e",
   "metadata": {},
   "source": [
    "# Multi-Class Review Ranker: Final Model Analysis\n",
    "\n",
    "This notebook presents the complete pipeline for the Multi-Class Review Ranker project. \n",
    "It covers:\n",
    "1.  **Data Preparation**: Loading, Cleaning, and EDA.\n",
    "2.  **Model Training**: Implementing and Tuning the 3 strategies.\n",
    "    *   **Naive Bayes (Tuned)**\n",
    "    *   **SVM (Tuned)**\n",
    "    *   **Word2Vec with POS-Lemmatization**\n",
    "3.  **Final Comparison**: Selecting the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a42537",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n",
    "Import necessary libraries and download NLTK resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6094b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfad5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configuration\n",
    "models_dir = '../saved_models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "print(f\"Models directory set to: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30709f19",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "Load the raw Amazon Reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3184bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/raw/Amazon_Reviews.csv'\n",
    "df = pd.read_csv(file_path, engine='python', on_bad_lines='skip')\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b0821",
   "metadata": {},
   "source": [
    "## 3. Missing Values Analysis\n",
    "Visualize missing data using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de215e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
    "plt.title('Missing Values Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cc51c9",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning & Rating Extraction\n",
    "Extract numeric ratings and drop rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef67465",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating_numeric'] = df['Rating'].str.extract(r'Rated (\\d) out of 5 stars').astype(float)\n",
    "\n",
    "# Drop rows with missing review text or rating\n",
    "df_clean = df.dropna(subset=['Review Text', 'rating_numeric']).copy()\n",
    "df_clean['rating_numeric'] = df_clean['rating_numeric'].astype(int)\n",
    "print(f\"Shape after dropping nulls: {df_clean.shape}\")\n",
    "\n",
    "# Rating Distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='rating_numeric', data=df_clean, palette='coolwarm')\n",
    "plt.title('Rating Distribution')\n",
    "plt.xlabel('Rating')\n",
    "plt.show()\n",
    "\n",
    "# Rating Proportions\n",
    "rating_counts = df_clean['rating_numeric'].value_counts().sort_index()\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%', colors=sns.color_palette('coolwarm', 5))\n",
    "plt.title('Rating Proportions')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRating value counts:\")\n",
    "print(rating_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c3209b",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "Extract reviewer experience, combine text fields, and create sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['reviewer_experience'] = df_clean['Review Count'].str.extract(r'(\\d+)').astype(float).fillna(1)\n",
    "\n",
    "# Combine Review Title + Review Text for richer content\n",
    "df_clean['full_text'] = df_clean['Review Title'].fillna('') + ' ' + df_clean['Review Text'].fillna('')\n",
    "df_clean['full_text'] = df_clean['full_text'].str.strip()\n",
    "\n",
    "# Create Sentiment Labels\n",
    "sentiment_map = {1: 'Very Bad', 2: 'Bad', 3: 'Neutral', 4: 'Good', 5: 'Very Good'}\n",
    "df_clean['sentiment_label'] = df_clean['rating_numeric'].map(sentiment_map)\n",
    "\n",
    "# Reviewer experience distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df_clean['reviewer_experience'], bins=50, kde=True, color='teal')\n",
    "plt.title('Reviewer Experience (# of Reviews)')\n",
    "plt.xlabel('Number of Reviews')\n",
    "plt.xlim(0, 100)\n",
    "plt.show()\n",
    "\n",
    "# Reviewer experience by rating\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='rating_numeric', y='reviewer_experience', data=df_clean, hue='rating_numeric', \n",
    "            palette='coolwarm', legend=False)\n",
    "plt.title('Reviewer Experience by Rating')\n",
    "plt.ylim(0, 50)\n",
    "plt.show()\n",
    "\n",
    "# Sentiment label distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='sentiment_label', data=df_clean, hue='sentiment_label',\n",
    "              order=['Very Bad', 'Bad', 'Neutral', 'Good', 'Very Good'], \n",
    "              palette='RdYlGn', legend=False)\n",
    "plt.title('Sentiment Label Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "print(\"New Features Added:\")\n",
    "print(f\"  - reviewer_experience: min={df_clean['reviewer_experience'].min():.0f}, max={df_clean['reviewer_experience'].max():.0f}, mean={df_clean['reviewer_experience'].mean():.1f}\")\n",
    "print(f\"  - full_text: Review Title + Review Text combined\")\n",
    "print(f\"  - sentiment_label: {list(sentiment_map.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54871b05",
   "metadata": {},
   "source": [
    "## 6. Text Length Analysis (Before Preprocessing)\n",
    "Analyze the distribution of character and word counts in the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1266407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['text_length'] = df_clean['Review Text'].str.len()\n",
    "df_clean['word_count'] = df_clean['Review Text'].str.split().str.len()\n",
    "\n",
    "# Review Length Distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df_clean['text_length'], bins=50, kde=True, color='steelblue')\n",
    "plt.title('Review Length Distribution (Characters)')\n",
    "plt.xlabel('Character Count')\n",
    "plt.show()\n",
    "\n",
    "# Word Count by Rating\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='rating_numeric', y='word_count', data=df_clean, palette='coolwarm')\n",
    "plt.title('Word Count by Rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.show()\n",
    "\n",
    "print(\"Text Length Statistics:\")\n",
    "print(df_clean[['text_length', 'word_count']].describe().round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b79d7a",
   "metadata": {},
   "source": [
    "## 7. Most Frequent Words (Before Preprocessing)\n",
    "Identify the top occuring words before any cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed560c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_before = ' '.join(df_clean['full_text'].astype(str)).lower().split()\n",
    "word_freq_before = Counter(all_words_before).most_common(25)\n",
    "\n",
    "words_df = pd.DataFrame(word_freq_before, columns=['word', 'count'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='count', y='word', data=words_df, palette='Blues_d')\n",
    "plt.title('Top 25 Words BEFORE Preprocessing')\n",
    "plt.xlabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488b4d5",
   "metadata": {},
   "source": [
    "## 8. Define Preprocessing Functions\n",
    "Setup NLTK tools and define the text cleaning function (lemmatization, stopword removal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf1308",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = set(words.words())\n",
    "for synset in wordnet.all_synsets():\n",
    "    for lemma in synset.lemmas():\n",
    "        english_words.add(lemma.name().lower().replace('_', ' '))\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Keep only letters\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    tokens = [w for w in tokens if w in english_words and len(w) > 2]  # English words only, min length 3\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Preprocessing function ready. English vocabulary size:\", len(english_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701549a6",
   "metadata": {},
   "source": [
    "## 9. Apply Preprocessing\n",
    "Run the preprocessing function on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df_clean['clean_text'] = df_clean['full_text'].progress_apply(preprocess_text)\n",
    "\n",
    "# Show example before/after\n",
    "print(\"Example transformation (Title + Review Text):\")\n",
    "print(f\"BEFORE: {df_clean['full_text'].iloc[0][:250]}...\")\n",
    "print(f\"\\nAFTER: {df_clean['clean_text'].iloc[0][:250]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b0df7",
   "metadata": {},
   "source": [
    "## 10. Most Frequent Words (After Preprocessing)\n",
    "Visualize the top words after cleaning to verify noise removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3ca33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_after = ' '.join(df_clean['clean_text'].astype(str)).split()\n",
    "word_freq_after = Counter(all_words_after).most_common(25)\n",
    "\n",
    "# Before\n",
    "words_before_df = pd.DataFrame(word_freq_before, columns=['word', 'count'])\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='count', y='word', data=words_before_df, hue='word', palette='Reds_d', legend=False)\n",
    "plt.title('Top 25 Words BEFORE Preprocessing')\n",
    "plt.show()\n",
    "\n",
    "# After\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='count', y='word', data=words_after_df, hue='word', palette='Greens_d', legend=False)\n",
    "plt.title('Top 25 Words AFTER Preprocessing')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVocabulary size before: {len(set(all_words_before))}\")\n",
    "print(f\"Vocabulary size after: {len(set(all_words_after))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f414fe",
   "metadata": {},
   "source": [
    "## 11. Text Length Comparison\n",
    "Compare text length statistics before and after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea55e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['clean_text_length'] = df_clean['clean_text'].str.len()\n",
    "df_clean['clean_word_count'] = df_clean['clean_text'].str.split().str.len()\n",
    "\n",
    "# Length comparison\n",
    "length_data = pd.DataFrame({\n",
    "    'Before': df_clean['text_length'],\n",
    "    'After': df_clean['clean_text_length']\n",
    "}).melt(var_name='Stage', value_name='Length')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=length_data, x='Length', hue='Stage', kde=True, alpha=0.5)\n",
    "plt.title('Character Count: Before vs After')\n",
    "plt.xlim(0, 2000)\n",
    "plt.show()\n",
    "\n",
    "# Word count by rating (after)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='rating_numeric', y='clean_word_count', data=df_clean, hue='rating_numeric', palette='coolwarm', legend=False)\n",
    "plt.title('Word Count by Rating (After Cleaning)')\n",
    "plt.show()\n",
    "\n",
    "# Stats comparison\n",
    "print(\"Text Length Statistics Comparison:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Before (chars)': df_clean['text_length'].describe(),\n",
    "    'After (chars)': df_clean['clean_text_length'].describe(),\n",
    "    'Before (words)': df_clean['word_count'].describe(),\n",
    "    'After (words)': df_clean['clean_word_count'].describe()\n",
    "}).round(1)\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e01b3ee",
   "metadata": {},
   "source": [
    "## 12. Top Words by Rating Class\n",
    "Analyze which words are most distinctive for each star rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd469fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#d73027', '#fc8d59', '#fee08b', '#91cf60', '#1a9850']\n",
    "\n",
    "for i, rating in enumerate([1, 2, 3, 4, 5]):\n",
    "    rating_text = ' '.join(df_clean[df_clean['rating_numeric'] == rating]['clean_text'])\n",
    "    top_words = Counter(rating_text.split()).most_common(10)\n",
    "    words_df = pd.DataFrame(top_words, columns=['word', 'count'])\n",
    "    \n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='count', y='word', data=words_df, color=colors[i])\n",
    "    plt.title(f'Top 10 Words - Rating {rating}')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880033a8",
   "metadata": {},
   "source": [
    "## 13. Advanced Feature Engineering\n",
    "Create features for punctuation usage, capitalization, and word length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['exclamation_count'] = df_clean['Review Text'].str.count('!')\n",
    "df_clean['question_count'] = df_clean['Review Text'].str.count('\\?')\n",
    "df_clean['uppercase_ratio'] = df_clean['Review Text'].apply(\n",
    "    lambda x: sum(1 for c in str(x) if c.isupper()) / max(len(str(x)), 1)\n",
    ")\n",
    "df_clean['avg_word_length'] = df_clean['clean_text'].apply(\n",
    "    lambda x: np.mean([len(w) for w in x.split()]) if len(x.split()) > 0 else 0\n",
    ")\n",
    "\n",
    "# Feature correlation heatmap\n",
    "feature_cols = ['clean_word_count', 'exclamation_count', 'question_count', 'uppercase_ratio', 'avg_word_length', 'rating_numeric']\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df_clean[feature_cols].corr(), annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd2a00",
   "metadata": {},
   "source": [
    "## 14. Feature Distributions\n",
    "Visualize how the new engineering features correlate with ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['exclamation_count', 'question_count', 'uppercase_ratio', 'avg_word_length']\n",
    "titles = ['Exclamation Count', 'Question Count', 'Uppercase Ratio', 'Avg Word Length']\n",
    "\n",
    "for feat, title in zip(features, titles):\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='rating_numeric', y=feat, data=df_clean, hue='rating_numeric', palette='coolwarm', legend=False)\n",
    "    plt.title(f'{title} by Rating')\n",
    "    plt.xlabel('Rating')\n",
    "    plt.show()\n",
    "\n",
    "# Stats per rating\n",
    "print(\"Feature Statistics by Rating:\")\n",
    "print(df_clean.groupby('rating_numeric')[features].mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c611bd",
   "metadata": {},
   "source": [
    "## 15. Final Data Cleaning\n",
    "Remove empty texts and duplicates generated during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e33054",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape before cleaning: {df_clean.shape}\")\n",
    "\n",
    "# Remove rows with empty clean_text\n",
    "df_clean = df_clean[df_clean['clean_text'].str.len() > 0].copy()\n",
    "print(f\"After removing empty texts: {df_clean.shape}\")\n",
    "\n",
    "# Remove duplicates\n",
    "n_duplicates = df_clean.duplicated(subset=['clean_text']).sum()\n",
    "df_clean = df_clean.drop_duplicates(subset=['clean_text'])\n",
    "print(f\"After removing {n_duplicates} duplicates: {df_clean.shape}\")\n",
    "\n",
    "# Final rating distribution\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.countplot(x='rating_numeric', data=df_clean, hue='rating_numeric', palette='coolwarm', legend=False)\n",
    "plt.title('Final Rating Distribution')\n",
    "\n",
    "# Add explicit labels to bars\n",
    "for i, v in enumerate(df_clean['rating_numeric'].value_counts().sort_index()):\n",
    "     ax.text(i, v + 100, str(v), ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3e816",
   "metadata": {},
   "source": [
    "## 16. Prepare Final Dataset\n",
    "Select final columns and rename for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f02debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = ['clean_text', 'rating_numeric', 'sentiment_label', 'reviewer_experience',\n",
    "              'clean_word_count', 'exclamation_count', 'question_count', \n",
    "              'uppercase_ratio', 'avg_word_length']\n",
    "df_final = df_clean[final_cols].copy()\n",
    "df_final.columns = ['text', 'rating', 'sentiment', 'reviewer_experience',\n",
    "                    'word_count', 'exclamation_count', 'question_count', \n",
    "                    'uppercase_ratio', 'avg_word_length']\n",
    "\n",
    "print(\"Final Dataset Summary:\")\n",
    "print(f\"Shape: {df_final.shape}\")\n",
    "print(f\"\\nSentiment Class Distribution:\")\n",
    "print(df_final['sentiment'].value_counts().reindex(['Very Bad', 'Bad', 'Neutral', 'Good', 'Very Good']))\n",
    "print(f\"\\nSample:\")\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7634d8f",
   "metadata": {},
   "source": [
    "## 17. Save Processed Data\n",
    "Save the clean dataset for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef24689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../data/processed/amazon_reviews_processed.csv'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "df_final.to_csv(output_path, index=False)\n",
    "print(f\"Saved processed data to: {output_path}\")\n",
    "\n",
    "# Final Summary Table\n",
    "summary = pd.DataFrame({\n",
    "    'Metric': ['Total Samples', 'Features', 'Vocabulary Size', 'Avg Text Length', 'Class Imbalance Ratio'],\n",
    "    'Value': [len(df_final), len(df_final.columns), len(set(' '.join(df_final['text']).split())),\n",
    "              f\"{df_final['word_count'].mean():.1f} words\", f\"{df_final['rating'].value_counts().max() / df_final['rating'].value_counts().min():.1f}x\"]\n",
    "})\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d669ea",
   "metadata": {},
   "source": [
    "## 18. Model Training Setup\n",
    "Import sklearn libraries and prepare for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02caf907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "import joblib\n",
    "\n",
    "print(\"Model training libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01676ed1",
   "metadata": {},
   "source": [
    "## 19. Train-Test Split\n",
    "Split data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = df_final['text']\n",
    "X_features = df_final[['reviewer_experience', 'word_count', 'exclamation_count', \n",
    "                       'question_count', 'uppercase_ratio', 'avg_word_length']]\n",
    "y = df_final['rating']\n",
    "\n",
    "X_text_train, X_text_test, X_feat_train, X_feat_test, y_train, y_test = train_test_split(\n",
    "    X_text, X_features, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_text_train)} samples\")\n",
    "print(f\"Test set: {len(X_text_test)} samples\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6190b",
   "metadata": {},
   "source": [
    "## 20. Hyperparameter Tuning Setup\n",
    "Prepare data for GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbcee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define helper function for 3-class mapping\n",
    "def map_to_3_classes(rating):\n",
    "    if rating <= 2:\n",
    "        return 'Negative'\n",
    "    elif rating == 3:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "# We'll tune on the 3-class problem since it showed better results\n",
    "# Using text data only for pipeline simplicity\n",
    "\n",
    "# Create a fresh train/test split for tuning\n",
    "X_tune = df_clean['clean_text']\n",
    "y_tune = df_clean['rating_numeric'].apply(map_to_3_classes)\n",
    "\n",
    "print(f\"Data shape: {len(X_tune)} samples\")\n",
    "print(f\"Class distribution:\\n{y_tune.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5a1b0",
   "metadata": {},
   "source": [
    "## 2. Models 1 & 2: Tuned Naive Bayes & SVM\n",
    "\n",
    "We will now perform Hyperparameter Tuning on our two baseline models to maximize their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b8646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create pipeline: TF-IDF + Naive Bayes\n",
    "nb_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "nb_param_grid = {\n",
    "    'tfidf__max_features': [3000, 5000, 8000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'tfidf__min_df': [1, 2, 3],\n",
    "    'tfidf__max_df': [0.9, 0.95, 1.0],\n",
    "    'clf__alpha': [0.01, 0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Use stratified k-fold for imbalanced data\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Tuning Naive Bayes... (this may take a few minutes)\")\n",
    "print(f\"Total combinations: {3*3*3*3*4} = 324\")\n",
    "\n",
    "nb_grid_search = GridSearchCV(\n",
    "    nb_pipeline,\n",
    "    nb_param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "nb_grid_search.fit(X_tune, y_tune)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NAIVE BAYES - BEST PARAMETERS:\")\n",
    "print(\"=\" * 60)\n",
    "for param, value in nb_grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest CV Accuracy: {nb_grid_search.best_score_:.4f} ({nb_grid_search.best_score_*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1698789",
   "metadata": {},
   "source": [
    "### 20.1 Naive Bayes Hyperparameter Tuning\n",
    "Use GridSearchCV to find optimal parameters for Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline: TF-IDF + SVM\n",
    "svm_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LinearSVC(max_iter=2000))\n",
    "])\n",
    "\n",
    "# Define parameter grid for SVM\n",
    "svm_param_grid = {\n",
    "    'tfidf__max_features': [3000, 5000, 8000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__min_df': [1, 2],\n",
    "    'clf__C': [0.1, 0.5, 1.0, 2.0],\n",
    "    'clf__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "print(\"Tuning SVM... (this may take several minutes)\")\n",
    "print(f\"Total combinations: {3*2*2*4*2} = 96\")\n",
    "\n",
    "svm_grid_search = GridSearchCV(\n",
    "    svm_pipeline,\n",
    "    svm_param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "svm_grid_search.fit(X_tune, y_tune)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SVM - BEST PARAMETERS:\")\n",
    "print(\"=\" * 60)\n",
    "for param, value in svm_grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest CV Accuracy: {svm_grid_search.best_score_:.4f} ({svm_grid_search.best_score_*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb520f",
   "metadata": {},
   "source": [
    "### 20.2 SVM Hyperparameter Tuning\n",
    "Use GridSearchCV to find optimal parameters for LinearSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23046e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data for final evaluation\n",
    "X_train_tune, X_test_tune, y_train_tune, y_test_tune = train_test_split(\n",
    "    X_tune, y_tune, test_size=0.2, random_state=42, stratify=y_tune\n",
    ")\n",
    "\n",
    "# Get best models\n",
    "best_nb = nb_grid_search.best_estimator_\n",
    "best_svm = svm_grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_nb_tuned = best_nb.predict(X_test_tune)\n",
    "y_pred_svm_tuned = best_svm.predict(X_test_tune)\n",
    "\n",
    "nb_tuned_acc = accuracy_score(y_test_tune, y_pred_nb_tuned)\n",
    "svm_tuned_acc = accuracy_score(y_test_tune, y_pred_svm_tuned)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TUNED MODELS - TEST SET PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nNAIVE BAYES (Tuned):\")\n",
    "print(f\"   Accuracy: {nb_tuned_acc:.4f} ({nb_tuned_acc*100:.2f}%)\")\n",
    "print(classification_report(y_test_tune, y_pred_nb_tuned))\n",
    "\n",
    "print(\"\\nSVM (Tuned):\")\n",
    "print(f\"   Accuracy: {svm_tuned_acc:.4f} ({svm_tuned_acc*100:.2f}%)\")\n",
    "print(classification_report(y_test_tune, y_pred_svm_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c3852e",
   "metadata": {},
   "source": [
    "### 20.3 Evaluate Tuned Models\n",
    "Test the best performing models on the hold-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4307504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tuned models\n",
    "# joblib.dump(best_nb, f'{models_dir}/naive_bayes_tuned.pkl')\n",
    "# joblib.dump(best_svm, f'{models_dir}/svm_tuned.pkl')\n",
    "\n",
    "# print(\"Tuned models saved:\")\n",
    "# print(f\"   - {models_dir}/naive_bayes_tuned.pkl\")\n",
    "# print(f\"   - {models_dir}/svm_tuned.pkl\")\n",
    "\n",
    "# Print final best parameters for reference\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL BEST HYPERPARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNaive Bayes:\")\n",
    "for param, value in nb_grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "    \n",
    "print(\"\\nSVM:\")\n",
    "for param, value in svm_grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffaf431",
   "metadata": {},
   "source": [
    "### 20.4 Save Best Tuned Models\n",
    "Serialize the best models to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77edd5c5",
   "metadata": {},
   "source": [
    "## 3. Model 3: Word2Vec (POS-Lemma Approach)\n",
    "\n",
    "We will explore semantic embeddings using Word2Vec, with a focus on improving text preprocessing using Part-of-Speech (POS) Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f72557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data for Word2Vec (needs list of list of tokens)\n",
    "# We use the text data split created earlier\n",
    "X_train_clean_text = X_text_train.astype(str)\n",
    "X_test_clean_text = X_text_test.astype(str)\n",
    "\n",
    "def tokenize_corpus(text_series):\n",
    "    return [text.split() for text in text_series]\n",
    "\n",
    "X_train_tokens = tokenize_corpus(X_train_clean_text)\n",
    "X_test_tokens = tokenize_corpus(X_test_clean_text)\n",
    "\n",
    "# Combine for training the embedding model (to see more vocabulary)\n",
    "full_corpus_tokens = X_train_tokens + X_test_tokens\n",
    "\n",
    "print(f\"Training Word2Vec on {len(full_corpus_tokens)} sentences...\")\n",
    "\n",
    "# Train Word2Vec Model\n",
    "# vector_size=100: Dimension of the dense vector\n",
    "# window=5: Context window size\n",
    "# min_count=2: Ignore rare words\n",
    "w2v_model = Word2Vec(sentences=full_corpus_tokens, vector_size=100, window=5, min_count=2, workers=4, seed=42)\n",
    "\n",
    "print(f\"Vocabulary size: {len(w2v_model.wv)}\")\n",
    "\n",
    "# Check similar words to 'good' and 'bad' to verify semantic learning\n",
    "print(\"\\nMost similar to 'good':\")\n",
    "try:\n",
    "    print(w2v_model.wv.most_similar('good', topn=5))\n",
    "except KeyError:\n",
    "    print(\"'good' not in vocab\")\n",
    "\n",
    "print(\"\\nMost similar to 'bad':\")\n",
    "try:\n",
    "    print(w2v_model.wv.most_similar('bad', topn=5))\n",
    "except KeyError:\n",
    "    print(\"'bad' not in vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c8b6a",
   "metadata": {},
   "source": [
    "## 22. Train Word2Vec Model\n",
    "Train custom embeddings on the review corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_vector(word_list, model):\n",
    "    # Filter words that are in the model's vocabulary\n",
    "    vectors = [model.wv[word] for word in word_list if word in model.wv]\n",
    "    \n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Calculate mean\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Vectorize Train and Test sets\n",
    "X_train_w2v = np.array([get_mean_vector(tokens, w2v_model) for tokens in X_train_tokens])\n",
    "X_test_w2v = np.array([get_mean_vector(tokens, w2v_model) for tokens in X_test_tokens])\n",
    "\n",
    "print(f\"Train features shape: {X_train_w2v.shape}\")\n",
    "print(f\"Test features shape: {X_test_w2v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f2215f",
   "metadata": {},
   "source": [
    "## 23. Create Document Vectors\n",
    "Average word vectors to create a single vector representation for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361aec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Determine target variable (prefer 3-class if available to match best models)\n",
    "target_train = y_train\n",
    "target_test = y_test\n",
    "\n",
    "# Check unique classes\n",
    "unique_classes = np.unique(target_train)\n",
    "print(f\"Current target classes: {unique_classes}\")\n",
    "\n",
    "if len(unique_classes) > 3:\n",
    "    print(\"Detected 5-class target. Attempting to use y_train_3class if available...\")\n",
    "    if 'y_train_3class' in locals() and len(y_train_3class) == len(X_train_w2v):\n",
    "        print(\"Using y_train_3class for training.\")\n",
    "        target_train = y_train_3class\n",
    "        # Ensure y_test match\n",
    "        if 'y_test_3class' in locals():\n",
    "            target_test = y_test_3class\n",
    "        else:\n",
    "            print(\"Warning: y_test_3class not found, using original y_test (might mismatch)\")\n",
    "    else:\n",
    "        print(\"y_train_3class not found or size mismatch. Creating temporary 3-class mapping...\")\n",
    "        # Simple mapping function if needed, or proceed with 5-class\n",
    "        # Assuming map_to_3_classes exists from earlier in notebook\n",
    "        try:\n",
    "             target_train = y_train.apply(map_to_3_classes)\n",
    "             target_test = y_test.apply(map_to_3_classes)\n",
    "             print(\"Mapped to 3 classes successfully.\")\n",
    "        except NameError:\n",
    "             print(\"map_to_3_classes generic function not found. Proceeding with original classes.\")\n",
    "\n",
    "# Logistic Regression is a strong baseline for dense embeddings\n",
    "lr_w2v = LogisticRegression(max_iter=2000, random_state=42)\n",
    "lr_w2v.fit(X_train_w2v, target_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_w2v = lr_w2v.predict(X_test_w2v)\n",
    "w2v_acc = accuracy_score(target_test, y_pred_w2v)\n",
    "\n",
    "print(f\"Word2Vec + Logistic Regression Accuracy: {w2v_acc:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(target_test, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64f7c2",
   "metadata": {},
   "source": [
    "## 24. Train Classifier on Embeddings\n",
    "Train a Logistic Regression model using the document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9918ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization: t-SNE of Word Vectors\n",
    "# We will visualize the top 100 most frequent words to see semantic clusters\n",
    "\n",
    "def plot_word_embeddings(model, topn=100):\n",
    "    # Get vocabulary and vectors\n",
    "    vocab_keys = list(model.wv.index_to_key)[:topn]\n",
    "    vectors = [model.wv[word] for word in vocab_keys]\n",
    "    \n",
    "    # Reduce dimensions to 2D\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, init='pca', learning_rate='auto')\n",
    "    vectors_2d = tsne.fit_transform(np.array(vectors))\n",
    "    \n",
    "    # Plot\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=vectors_2d[:, 0], y=vectors_2d[:, 1], s=100, alpha=0.7)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, word in enumerate(vocab_keys):\n",
    "        plt.annotate(word, xy=(vectors_2d[i, 0], vectors_2d[i, 1]), \n",
    "                     xytext=(5, 2), textcoords='offset points', \n",
    "                     ha='right', va='bottom', fontsize=9)\n",
    "        \n",
    "    plt.title(f't-SNE Visualization of Top {topn} Word Embeddings', fontsize=16)\n",
    "    plt.xlabel('t-SNE dimension 1')\n",
    "    plt.ylabel('t-SNE dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_word_embeddings(w2v_model, topn=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a8e8f",
   "metadata": {},
   "source": [
    "## 25. Visualize Word Embeddings\n",
    "Use t-SNE to visualize semantic clusters of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd8077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Ensure we have the tagger\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def improved_preprocess(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    tokens = text.split()\n",
    "    \n",
    "    if not tokens:\n",
    "        return \"\"\n",
    "        \n",
    "    # POS Tagging (Context aware)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    final_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        # Filter first\n",
    "        if word not in stop_words and len(word) > 2 and word in english_words:\n",
    "            # Lemmatize with tag\n",
    "            wn_tag = get_wordnet_pos(tag)\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            final_tokens.append(lemma)\n",
    "            \n",
    "    return ' '.join(final_tokens)\n",
    "\n",
    "# Demonstration of the fix\n",
    "test_samples = [\n",
    "    \"I received the package yesterday and I am receiving another one today.\",\n",
    "    \"This is the best product I have ever got.\",\n",
    "    \"She was saying that it is bad and worse.\"\n",
    "]\n",
    "\n",
    "print(\"--- Discrepancy Check ---\")\n",
    "for sample in test_samples:\n",
    "    print(f\"\\nOriginal: {sample}\")\n",
    "    # Using the old simple lemmatizer approach (assuming default noun)\n",
    "    old_tokens = [lemmatizer.lemmatize(w) for w in sample.lower().split() if w in english_words] \n",
    "    print(f\"Old (Naive): {' '.join(old_tokens)}\")\n",
    "    \n",
    "    # New approach\n",
    "    new_text = improved_preprocess(sample)\n",
    "    print(f\"New (POS):   {new_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c76802",
   "metadata": {},
   "source": [
    "## 26. Improve Lemmatization (POS-Aware)\n",
    "Implement smarter preprocessing that respects Part-of-Speech tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fbc202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply directly to the split data (handling Series or potential Array)\n",
    "if hasattr(X_text_train, 'apply'):\n",
    "    X_train_clean_v2 = X_text_train.progress_apply(improved_preprocess)\n",
    "    X_test_clean_v2 = X_text_test.progress_apply(improved_preprocess)\n",
    "else:\n",
    "    # Fallback if it's a numpy array\n",
    "    import pandas as pd\n",
    "    X_train_clean_v2 = pd.Series(X_text_train).progress_apply(improved_preprocess)\n",
    "    X_test_clean_v2 = pd.Series(X_text_test).progress_apply(improved_preprocess)\n",
    "\n",
    "X_train_tokens_v2 = [text.split() for text in X_train_clean_v2]\n",
    "X_test_tokens_v2 = [text.split() for text in X_test_clean_v2]\n",
    "full_corpus_tokens_v2 = X_train_tokens_v2 + X_test_tokens_v2\n",
    "\n",
    "# Re-train Word2Vec\n",
    "print(\"Training Word2Vec V2...\")\n",
    "w2v_model_v2 = Word2Vec(sentences=full_corpus_tokens_v2, vector_size=100, window=5, min_count=2, workers=4, seed=42)\n",
    "\n",
    "print(\"New Vocabulary size:\", len(w2v_model_v2.wv))\n",
    "\n",
    "# Re-check relationships\n",
    "print(\"\\nMost similar to 'good' (V2):\")\n",
    "try:\n",
    "    print(w2v_model_v2.wv.most_similar('good', topn=5))\n",
    "except KeyError:\n",
    "    print(\"'good' not in vocab\")\n",
    "\n",
    "# Generate vectors for Classifier using V2\n",
    "X_train_w2v_v2 = np.array([get_mean_vector(tokens, w2v_model_v2) for tokens in X_train_tokens_v2])\n",
    "X_test_w2v_v2 = np.array([get_mean_vector(tokens, w2v_model_v2) for tokens in X_test_tokens_v2])\n",
    "\n",
    "# Train LR V2\n",
    "lr_w2v_v2 = LogisticRegression(max_iter=2000, random_state=42)\n",
    "lr_w2v_v2.fit(X_train_w2v_v2, target_train) # target_train defined in previous steps\n",
    "\n",
    "y_pred_w2v_v2 = lr_w2v_v2.predict(X_test_w2v_v2)\n",
    "w2v_acc_v2 = accuracy_score(target_test, y_pred_w2v_v2)\n",
    "\n",
    "print(f\"\\nWord2Vec (POS Lemmatized) Accuracy: {w2v_acc_v2:.4f}\")\n",
    "\n",
    "# Save Word2Vec Logistic Regression Model\n",
    "# joblib.dump(lr_w2v_v2, f'{models_dir}/word2vec_lr.pkl')\n",
    "# print(f\"Word2Vec Logistic Regression model saved to: {models_dir}/word2vec_lr.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcbcda8",
   "metadata": {},
   "source": [
    "## 27. Re-train Word2Vec with Improved Processing\n",
    "Apply the new preprocessing and re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc073b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score, balanced_accuracy_score, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# 1. Detailed Metric Calculation\n",
    "def get_metrics(name, y_true, y_pred):\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Balanced Accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        'F1 (Weighted)': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'F1 (Macro)': f1_score(y_true, y_pred, average='macro'),\n",
    "        'Precision (Weighted)': precision_score(y_true, y_pred, average='weighted'),\n",
    "        'Recall (Weighted)': recall_score(y_true, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "# Gather metrics for all models\n",
    "metrics_data = [\n",
    "    get_metrics('Naive Bayes (Tuned)', y_test_tune, y_pred_nb_tuned),\n",
    "    get_metrics('SVM (Tuned)', y_test_tune, y_pred_svm_tuned),\n",
    "    get_metrics('Word2Vec (POS-Lemma)', target_test, y_pred_w2v_v2)\n",
    "]\n",
    "\n",
    "final_results = pd.DataFrame(metrics_data).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL MODEL PERFORMANCE REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(final_results.round(4).to_string(index=False))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Visualizations\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# A. Accuracy & Metrics Comparison\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(12, 6))\n",
    "long_results = final_results.melt(id_vars='Model', value_vars=['Accuracy', 'F1 (Weighted)'], \n",
    "                                  var_name='Metric', value_name='Score')\n",
    "ax = sns.barplot(x='Model', y='Score', hue='Metric', data=long_results, palette='viridis')\n",
    "plt.title('Model Comparison: Key Metrics', fontsize=16)\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "\n",
    "# Add value labels to bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.2f', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# B. Confusion Matrices\n",
    "def plot_cm(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    labels = sorted(list(set(y_true)))\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "plot_cm(y_test_tune, y_pred_nb_tuned, 'Naive Bayes (Tuned) - Confusion Matrix')\n",
    "plot_cm(y_test_tune, y_pred_svm_tuned, 'SVM (Tuned) - Confusion Matrix')\n",
    "plot_cm(target_test, y_pred_w2v_v2, 'Word2Vec (POS-Lemma) - Confusion Matrix')\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Feature Importance (SVM)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE (Predicted by SVM)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "feature_names = best_svm.named_steps['tfidf'].get_feature_names_out()\n",
    "svm_coefs = best_svm.named_steps['clf'].coef_\n",
    "classes = best_svm.named_steps['clf'].classes_\n",
    "\n",
    "for i, class_label in enumerate(classes):\n",
    "    # Sort coefficients\n",
    "    top10_indices = np.argsort(svm_coefs[i])[-10:]\n",
    "    top10_words = feature_names[top10_indices]\n",
    "    top10_weights = svm_coefs[i][top10_indices]\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=top10_weights, y=top10_words, hue=top10_words, legend=False, palette='magma')\n",
    "    plt.title(f'Top 10 Indicative Words for Class: {class_label} (SVM)', fontsize=14)\n",
    "    plt.xlabel('Coefficient Magnitude')\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. ROC / AUC Curves (Micro-Average)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ROC Curves Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Binarize labels\n",
    "y_test_bin = label_binarize(y_test_tune, classes=classes)\n",
    "\n",
    "# Get Scores\n",
    "y_score_nb = best_nb.predict_proba(X_test_tune)\n",
    "y_score_svm = best_svm.decision_function(X_test_tune) # SVM LinearSVC uses decision_function\n",
    "y_score_w2v = lr_w2v_v2.predict_proba(X_test_w2v_v2) \n",
    "# Note: W2V target_test must match classes order. Assuming consistency.\n",
    "\n",
    "def get_micro_curve(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true.ravel(), y_score.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "fpr_nb, tpr_nb, auc_nb = get_micro_curve(y_test_bin, y_score_nb)\n",
    "fpr_svm, tpr_svm, auc_svm = get_micro_curve(y_test_bin, y_score_svm)\n",
    "fpr_w2v, tpr_w2v, auc_w2v = get_micro_curve(y_test_bin, y_score_w2v) # Reuse y_test_bin as it is the same split if seeds preserved\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(fpr_nb, tpr_nb, label=f'Naive Bayes (Micro AUC = {auc_nb:.3f})', linewidth=2)\n",
    "plt.plot(fpr_svm, tpr_svm, label=f'SVM (Micro AUC = {auc_svm:.3f})', linewidth=2)\n",
    "plt.plot(fpr_w2v, tpr_w2v, label=f'Word2Vec (Micro AUC = {auc_w2v:.3f})', linewidth=2, linestyle='--')\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Micro-Average ROC Curve Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. Misclassification Analysis (Error Analysis)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISCLASSIFICATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze the best model\n",
    "best_model_name = final_results.iloc[0]['Model']\n",
    "print(f\"Top Model Identified: {best_model_name}\")\n",
    "\n",
    "if 'SVM' in best_model_name:\n",
    "    y_pred, X_src, y_true = y_pred_svm_tuned, X_test_tune, y_test_tune\n",
    "elif 'Naive' in best_model_name:\n",
    "    y_pred, X_src, y_true = y_pred_nb_tuned, X_test_tune, y_test_tune\n",
    "else:\n",
    "    y_pred, X_src, y_true = y_pred_w2v_v2, X_test_clean_v2, target_test\n",
    "\n",
    "# Find errors\n",
    "error_indices = np.where(y_pred != y_true)[0]\n",
    "print(f\"Total Misclassified Samples: {len(error_indices)} out of {len(y_true)}\")\n",
    "\n",
    "if len(error_indices) > 0:\n",
    "    # Sample 5 random errors\n",
    "    np.random.seed(99)\n",
    "    sample_erros = np.random.choice(error_indices, min(5, len(error_indices)), replace=False)\n",
    "    \n",
    "    error_records = []\n",
    "    # Need to handle pandas indices alignment\n",
    "    for i in sample_erros:\n",
    "        # Access via integer position (iloc) for both Series/Arrays\n",
    "        text_val = X_src.iloc[i] if hasattr(X_src, 'iloc') else X_src[i]\n",
    "        true_val = y_true.iloc[i] if hasattr(y_true, 'iloc') else y_true[i]\n",
    "        pred_val = y_pred[i]\n",
    "        \n",
    "        error_records.append({\n",
    "            'Review Text (Processed)': text_val, \n",
    "            'True Label': true_val, \n",
    "            'Predicted': pred_val\n",
    "        })\n",
    "    \n",
    "    error_df = pd.DataFrame(error_records)\n",
    "    print(\"\\nSample Errors:\")\n",
    "    pd.set_option('display.max_colwidth', 120)\n",
    "    print(error_df)\n",
    "    pd.reset_option('display.max_colwidth')\n",
    "\n",
    "best_model = final_results.iloc[0]\n",
    "print(f\"\\nCONCLUSION: The best performing model is {best_model['Model']} with {best_model['Accuracy']:.2%} accuracy.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cea4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define models directory\n",
    "models_dir = '../saved_models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING FINAL MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Naive Bayes\n",
    "joblib.dump(best_nb, f'{models_dir}/naive_bayes_final.pkl')\n",
    "print(f\"Naive Bayes saved to: {models_dir}/naive_bayes_final.pkl\")\n",
    "\n",
    "# 2. SVM\n",
    "joblib.dump(best_svm, f'{models_dir}/svm_final.pkl')\n",
    "print(f\"SVM saved to: {models_dir}/svm_final.pkl\")\n",
    "\n",
    "# 3. Word2Vec Classifier + Embedding Model\n",
    "joblib.dump(lr_w2v_v2, f'{models_dir}/word2vec_lr_final.pkl')\n",
    "w2v_model_v2.save(f'{models_dir}/word2vec.model')\n",
    "print(f\"Word2Vec Classifier saved to: {models_dir}/word2vec_lr_final.pkl\")\n",
    "print(f\"Word2Vec Embeddings saved to: {models_dir}/word2vec.model\")\n",
    "\n",
    "print(\"\\nAll models have been serialized and are ready for deployment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
